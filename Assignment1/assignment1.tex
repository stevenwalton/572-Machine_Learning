\documentclass[12pt,letter]{article}
\usepackage{geometry}\geometry{top=0.75in}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{xcolor}	% Color words
\usepackage{cancel}	% Crossing parts of equations out
\usepackage{tikz}    	% Drawing 
\usepackage{pgfplots}   % Other plotting
\usepgfplotslibrary{colormaps,fillbetween}
\usepackage{placeins}   % Float barrier
\usepackage{hyperref}   % Links
\usepackage{tikz-qtree} % Trees

%\tikzset{
%    treenode/.style = {shape=rectangle, rounded corners, draw, align=center}
%    root/.style     = {treenode, font=\Large}
%    env/.style      = {treenode, font=\ttyfamily\normalsize},
%    dummy/.style    = {circle,draw}
%}

%\tikzset{every tree node/.style={circle,align=center, anchor=west, grow=right}
%	}
\tikzset{every tree node/.style={align=center,minimum width=2em},%, draw},%, circle},
	 grow=right,
	 level distance=1.75cm}

% Don't indent
\setlength{\parindent}{0pt}
% Function to replace \section with a problem name specifically formatted
\newcommand{\problem}[1]{\vspace{3mm}\Large\textbf{{Problem {#1}\vspace{3mm}}}\normalsize\\}
% Formatting function, like \problem
\newcommand{\ppart}[1]{\vspace{2mm}\large\textbf{\\Part {#1})\vspace{2mm}}\normalsize\\}
% Formatting 
\newcommand{\condition}[1]{\vspace{1mm}\textbf{{#1}:}\normalsize\\}

\begin{document}
\title{CIS 572 Assignment 1}
\author{Steven Walton}
\maketitle
\problem{1}
Answer Exercise 3.1 from 
\href{https://www.cs.princeton.edu/courses/archive/spr07/cos424/papers/mitchell-dectrees.pdf}{Chapter 3 of Mitchell's machine learning book.}

\ppart{a} 
$$ A \wedge\neg B$$
\begin{tikzpicture}
%\Tree [.A [.T [.B [.T 0 ] [.F 1 ] ] ] [.F 0 ] ]
\Tree [.A [.T [.B [.T 0 ] [.F 1 ] ] ] [.F 0 ] ]
\end{tikzpicture}


\ppart{b}
$$A\vee (B\wedge C)$$
\begin{tikzpicture}
\Tree [.A [.T 1 ] [.F [.B [.T [.C [.T 1 ] [.F 0 ] ] ] [.F 0 ] ] ] ]

\end{tikzpicture}


\ppart{c}
$$ A\ \mathsf{XOR}\ B $$
\begin{tikzpicture}
\Tree [.A [.T [.B [.T 0 ] [.F 1 ] ] ] [.F [.B [.T 1 ] [.F 0 ] ] ] ]
\end{tikzpicture}

\ppart{d}
$$ (A\wedge B)\vee(C\wedge D)$$
\begin{tikzpicture}
\Tree [.A [.T [.B [.T 1 ] [.F 0 ] ] ] [.F [.C [.T [.D [.T 1 ] [.F 0 ] ] ] [.F 0 ] ] ] ]
\end{tikzpicture}

\problem{2}
Consider the samples in the Play-tennis dataset from Table $3.2$ in Mitchell's
textbook. If you calculate the information-gain for all of the attributes of this
set, you will observe that the attribute ``Outlook" has the largest information-
gain, which is equal to $0.246$. Therefore, the attribute ``Outlook" is the 
best heuristic choice for the root node.
\\
(a) List the labels of the new tree branches below the root node.
\\
(b) Which partition of the data will be assigned to each branch by ID3? Please
list the sample IDs that will be assigned to each branch.
\\
(c) Calculate the information gain for the remaining attributes in each branch, 
and determine which attribute will be chosen as the root of the sub-tree in
each branch.

\ppart{a}
Now that we know that Overcast has the highest information gain, of $0.246$, we
will use it as the root node. We will then create the next nodes and show their
corresponding values for playing tennis or not, in the form of $\big[+yes,-no\big]$

\begin{tikzpicture}[grow=down,level distance=3cm]
\Tree [.{Outlook\\$\big[+9,-6\big]$} [.{Sunny\\$\big[+2,-3\big]$} ] [.{Overcast\\$\big[+4,-0\big]$} ] [.{Rain\\$\big[+3,-2\big]$} ] ]
\end{tikzpicture}

\ppart{b}
To determine how we will partition the data we will look at the entropy of each
node.

Given sunny, we have:

Humidity: High $\big[+0,-3\big]$, Normal $\big[+2,-0\big]$

Temperature: High $\big[+0,-2\big]$, Mild $\big[+1,-1\big]$, Cold $\big[+1,-0\big]$

Wind: Strong $\big[+1,-1\big]$, Weak $\big[+1,-2\big]$

\begin{align*}
S(Sunny,Humidity) &= S_{sunny} - S_{sunny,high} - S_{sunny,low}\\
	          &= S_{sunny} - \frac35(-1\log_2{1}) - \frac25(-1\log_2{1})\\
		  &= 0.97 - \frac35 0 - \frac25 0\\
		  &= 0.97
\end{align*}
We'll shorten steps from now on. We note that any $\big[+a,-a\big]=1$ and 
$\big[+a,0\big] = \big[0,-a\big] = 0$
\begin{align*}
S(Sunny,Temperature) &= S_{sunny} - S_{sunny,hot} - S_{sunny,mild} - S_{sunny,cold}\\
		     &= 0.97 - \frac250 - \frac251 - \frac250\\
		     &= 0.57
\end{align*}
\begin{align*}
S(Sunny,Wind) &= S_{sunny} - S_{sunny,strong} - S_{sunny,weak}\\
	      &= 0.97 - \frac251 - \frac35\left(-\frac13\log_2{\frac13} - \frac23\log_2{\frac23}\right)\\
	      &= 0.019
\end{align*}

From here we can see that the best thing to split on is humidity and the worst 
is wind. That gives us the following update to our tree.

\begin{tikzpicture}[grow=down,level distance=3cm]
\Tree [.{Outlook\\$\big[+9,-6\big]$} [.{Sunny\\$\big[+2,-3\big]$} [.{Humidity\\$\big[+2,-3\big]$} ] ] [.{Overcast\\$\big[+4,-0\big]$} ] [.{Rain\\$\big[+3,-2\big]$} ] ]
\end{tikzpicture}

\ppart{c}
Continuing with this process we get the following tree

\begin{tikzpicture}[grow=down,level distance=3cm]
\Tree [.{Outlook\\$\big[+9,-6\big]$}
	[.{Sunny\\$\big[+2,-3\big]$}
	  [.{Humidity\\$\big[+2,-3\big]$} {High\\Yes} {Low\\No} ] 
        ]
	[.{Overcast\\$\big[+4,-0\big]$} Yes ]
	[.{Rain\\$\big[+3,-2\big]$}
	  [.{Wind\\$\big[+3,-2\big]$} {Strong\\No} {Weak\\Yes} ]
	]
      ]
\end{tikzpicture}

\problem{3}
Suppose a bank makes loan decisions using two decision trees, one that uses
attributes related to credit history and one that uses other demographic 
attributes. Each decision tree separately classifies a loan application as 
``High Risk" or ``Low Risk". The bank only offers a loan when both decision
trees predict ``Low Risk"

(a) Describe an algorithm for converting this pair of decision trees into a 
single decision tree that makes the same predictions (that is, it predicts
non-risky only when both of the original decision trees would have predicted 
non-risky).

(b) Let $n_1$ and $n_2$ be the number of leaves in the first and second decision
trees, respectively. Provide an upper bound on $n$, the number of leaves in the 
single equivalent decision tree, expressed as a function of $n_1$ and $n_2$.



\end{document}
